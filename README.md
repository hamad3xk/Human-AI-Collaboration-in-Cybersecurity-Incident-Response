Human–AI Collaboration in Cybersecurity Incident Response

Evaluating LLM-Generated Explanations for Trust and Decision-Making

📌 Overview

This repository contains the code, data, and analysis materials for my MSc project on Human–AI collaboration in cybersecurity incident response (CSIR).
The project investigates whether GPT-4–generated explanations for alerts can improve trust, decision-making speed, and alert acceptance in simulated CSIR tasks.

The experiment compared two conditions:

Condition A (Standard AI): Alerts without explanations

Condition B (Explainable AI): Same alerts with short, natural language explanations generated by GPT-4

📂 Repository Structure
```
human-ai-csir/
│
├── stimuli/                  # Synthetic alerts & GPT-4 explanations
│   ├── alerts.json
│   ├── explanations.json
│
├── app/                      # Web experiment (Flask + JS/HTML)
│   ├── app.py
│   ├── static/
│   ├── templates/
│
├── surveys/                  # Consent form & questionnaires
│   ├── consent_form.pdf
│   ├── pre_task_survey.md
│   ├── post_task_survey.md
│
├── analysis/                 # Jupyter notebooks & figures
│   ├── results.ipynb
│   ├── figures/
│   │   ├── trust_boxplot.png
│   │   ├── decision_time.png
│   │   ├── acceptance_rate.png
│
├── data/                     # Anonymized participant logs
│   ├── sample_data.csv
│
├── docs/                     # Extra documentation (optional)
│   ├── methodology.md
│   ├── study_design_diagram.png
│
├── README.md                 # Project overview & instructions
├── requirements.txt          # Python dependencies
└── LICENSE                   # License (MIT/Apache)

```

⚙️ Installation

Clone the repo and install dependencies:
```
git clone https://github.com/your-username/human-ai-csir.git
cd human-ai-csir
pip install -r requirements.txt
```

▶️ Running the Experiment

1. Start the Flask app:
```
python app/app.py
```
2. Open your browser at http://127.0.0.1:5000/
3. Complete the task trials (Condition A or B).
4. Logs will be stored in data/ as CSV.

📊 Analysis

The analysis/results.ipynb notebook contains:

* Descriptive statistics
* Independent-samples t-tests / Mann–Whitney tests
* Effect sizes & confidence intervals
* Boxplots and bar charts
Exported figures are saved in analysis/figures/.

🔍 Results (Pilot Study)

* Trust: Higher in the Explainable AI condition (significant, large effect).
* Decision Time: Faster with explanations (medium-to-large effect, marginal $p$).
* Acceptance Rate: Higher with explanations (medium effect, not fully significant).

🙏 Acknowledgments

* University of Adelaide, MSc in Cybersecurity
* Participants who volunteered for the pilot study
* OpenAI GPT-4 API for explanation generation